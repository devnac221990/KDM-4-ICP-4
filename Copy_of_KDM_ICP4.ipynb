{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of KDM_ICP4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devnac221990/KDM-4-ICP-4/blob/main/Copy_of_KDM_ICP4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqjcwzS24Jr_"
      },
      "source": [
        "Setting up PySpark in Colab\r\n",
        "\r\n",
        "Spark is written in the Scala programming language and requires the Java Virtual Machine (JVM) to run. Therefore, our first task is to download Java"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iGswToj4LIS",
        "outputId": "e3b2a55a-ebee-4540-e758-e64d31725a1d"
      },
      "source": [
        "!sudo apt-get update\r\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rIgn:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Waiting f\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\r0% [Waiting for headers] [Waiting for headers] [2 InRelease 0 B/3,626 B 0%] [Wa\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Waiting f\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rGet:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r                                                                               \rGet:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [4 InRelease 14.2 kB/88.7 k\r                                                                               \rIgn:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [4 InRelease 14.2 kB/88.7 k\r                                                                               \rHit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [45.4 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,732 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,929 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,360 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,391 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,162 kB]\n",
            "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [886 kB]\n",
            "Fetched 10.8 MB in 2s (4,460 kB/s)\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzRtNPkq5VIR"
      },
      "source": [
        "Next, we will install Apache Spark 3.0.1 with Hadoop 2.7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBYZfbpk34H3"
      },
      "source": [
        "!wget -q https://www-us.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L21HeCi5bde"
      },
      "source": [
        "Now, we just need to unzip that folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-D2QoWK5cEg"
      },
      "source": [
        "!tar xf spark-3.0.1-bin-hadoop2.7.tgz"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grIssZqC8aYj"
      },
      "source": [
        "There is one last thing that we need to install and that is the findspark library. It will locate Spark on the system and import it as a regular library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wke-MaxX57M9"
      },
      "source": [
        "!pip install -q findspark"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AJ1RV578hE9"
      },
      "source": [
        "Now that we have installed all the necessary dependencies in Colab, it is time to set the environment path. This will enable us to run Pyspark in the Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvMEW0ol59ts"
      },
      "source": [
        "import os\r\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\r\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weGRBVLK8nKb"
      },
      "source": [
        "We need to locate Spark in the system. For that, we import findspark and use the findspark.init() method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSleoMKz6Ecg"
      },
      "source": [
        "import findspark\r\n",
        "findspark.init()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6gW5avs8yWa"
      },
      "source": [
        "If you want to know the location where Spark is installed, use findspark.find()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "a3EHL6x76KcK",
        "outputId": "579c46cd-5bc0-4037-96c4-5b476e2e1913"
      },
      "source": [
        "findspark.find()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/spark-3.0.1-bin-hadoop2.7'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI0EI6Ox87wT"
      },
      "source": [
        "Now, we can import SparkSession from pyspark.sql and create a SparkSession, which is the entry point to Spark.\r\n",
        "\r\n",
        "You can give a name to the session using appName() and add some configurations with config() if you wish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcGOvWMJ6PUo"
      },
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "\r\n",
        "spark = SparkSession.builder\\\r\n",
        "        .master(\"local\")\\\r\n",
        "        .appName(\"Colab\")\\\r\n",
        "        .config('spark.ui.port', '4050')\\\r\n",
        "        .getOrCreate()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djWQ29kL9Ogy"
      },
      "source": [
        "Finally, print the SparkSession variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "GvcENqE16XEj",
        "outputId": "623550b6-8eb5-4847-8466-914bac7fe3a3"
      },
      "source": [
        "spark"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://fbe3aa3dcf64:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f51492bfbe0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4btlV8p6b3Q",
        "outputId": "c7ec553e-afbd-4df5-dbcc-2cd07b927123"
      },
      "source": [
        "#we need to load the dataset. We will use the read.csv module. \r\n",
        "#The inferSchema parameter provided will enable Spark to automatically determine the data type for each column but it has to go over the data once.\r\n",
        "# If you don’t want that to happen, then you can instead provide the schema explicitly in the schema parameter.\r\n",
        "\r\n",
        "df = spark.read.csv(\"/content/drive/MyDrive/data.csv\", header=True, inferSchema= True)\r\n",
        "df.printSchema()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- customerID: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- SeniorCitizen: integer (nullable = true)\n",
            " |-- Partner: string (nullable = true)\n",
            " |-- Dependents: string (nullable = true)\n",
            " |-- tenure: integer (nullable = true)\n",
            " |-- PhoneService: string (nullable = true)\n",
            " |-- MultipleLines: string (nullable = true)\n",
            " |-- InternetService: string (nullable = true)\n",
            " |-- OnlineSecurity: string (nullable = true)\n",
            " |-- OnlineBackup: string (nullable = true)\n",
            " |-- DeviceProtection: string (nullable = true)\n",
            " |-- TechSupport: string (nullable = true)\n",
            " |-- StreamingTV: string (nullable = true)\n",
            " |-- StreamingMovies: string (nullable = true)\n",
            " |-- Contract: string (nullable = true)\n",
            " |-- PaperlessBilling: string (nullable = true)\n",
            " |-- PaymentMethod: string (nullable = true)\n",
            " |-- MonthlyCharges: double (nullable = true)\n",
            " |-- TotalCharges: string (nullable = true)\n",
            " |-- Churn: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKJ7inwfxV5W",
        "outputId": "140d04ed-c685-4963-8d9f-0a1cf8d07089"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAZQLYf-tobz"
      },
      "source": [
        "# New section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0U-Tk7V7NC5",
        "outputId": "68492288-30ed-404c-99d3-2220c6a07004"
      },
      "source": [
        "df.show(5, truncate=False)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+-------------------------+--------------+------------+-----+\n",
            "|customerID|gender|SeniorCitizen|Partner|Dependents|tenure|PhoneService|MultipleLines   |InternetService|OnlineSecurity|OnlineBackup|DeviceProtection|TechSupport|StreamingTV|StreamingMovies|Contract      |PaperlessBilling|PaymentMethod            |MonthlyCharges|TotalCharges|Churn|\n",
            "+----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+-------------------------+--------------+------------+-----+\n",
            "|7590-VHVEG|Female|0            |Yes    |No        |1     |No          |No phone service|DSL            |No            |Yes         |No              |No         |No         |No             |Month-to-month|Yes             |Electronic check         |29.85         |29.85       |No   |\n",
            "|5575-GNVDE|Male  |0            |No     |No        |34    |Yes         |No              |DSL            |Yes           |No          |Yes             |No         |No         |No             |One year      |No              |Mailed check             |56.95         |1889.5      |No   |\n",
            "|3668-QPYBK|Male  |0            |No     |No        |2     |Yes         |No              |DSL            |Yes           |Yes         |No              |No         |No         |No             |Month-to-month|Yes             |Mailed check             |53.85         |108.15      |Yes  |\n",
            "|7795-CFOCW|Male  |0            |No     |No        |45    |No          |No phone service|DSL            |Yes           |No          |Yes             |Yes        |No         |No             |One year      |No              |Bank transfer (automatic)|42.3          |1840.75     |No   |\n",
            "|9237-HQITU|Female|0            |No     |No        |2     |Yes         |No              |Fiber optic    |No            |No          |No              |No         |No         |No             |Month-to-month|Yes             |Electronic check         |70.7          |151.65      |Yes  |\n",
            "+----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+-------------------------+--------------+------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "968rC6spvLNN",
        "outputId": "bc4c6ecb-c764-4774-8a17-f280425dd10d"
      },
      "source": [
        "simpleData = [(\"7590-VHVEG\",29.85,29.85), \\\r\n",
        "              (\"5575-GNVDE\",56.95,1889.5),\\\r\n",
        "              (\"3668-QPYBK\",53.85,108.15), \\\r\n",
        "            ]\r\n",
        "columns = [\"customerID\",\"MonthlyCharges\",\"TotalCharges\"]\r\n",
        "df = spark.createDataFrame(data = simpleData, schema = columns)\r\n",
        "df.printSchema()\r\n",
        "df.show(truncate=False)\r\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- customerID: string (nullable = true)\n",
            " |-- MonthlyCharges: double (nullable = true)\n",
            " |-- TotalCharges: double (nullable = true)\n",
            "\n",
            "+----------+--------------+------------+\n",
            "|customerID|MonthlyCharges|TotalCharges|\n",
            "+----------+--------------+------------+\n",
            "|7590-VHVEG|29.85         |29.85       |\n",
            "|5575-GNVDE|56.95         |1889.5      |\n",
            "|3668-QPYBK|53.85         |108.15      |\n",
            "+----------+--------------+------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_e0eoTu5YHC",
        "outputId": "af65b226-9306-4ff2-916b-ffbb5507d7a2"
      },
      "source": [
        "df.sort(df.MonthlyCharges.asc(),df.customerID.asc()).show(truncate=False)\r\n",
        "df.orderBy(df.MonthlyCharges.asc(),df.customerID.asc()).show(truncate=False)\r\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+--------------+------------+\n",
            "|customerID|MonthlyCharges|TotalCharges|\n",
            "+----------+--------------+------------+\n",
            "|7590-VHVEG|29.85         |29.85       |\n",
            "|3668-QPYBK|53.85         |108.15      |\n",
            "|5575-GNVDE|56.95         |1889.5      |\n",
            "+----------+--------------+------------+\n",
            "\n",
            "+----------+--------------+------------+\n",
            "|customerID|MonthlyCharges|TotalCharges|\n",
            "+----------+--------------+------------+\n",
            "|7590-VHVEG|29.85         |29.85       |\n",
            "|3668-QPYBK|53.85         |108.15      |\n",
            "|5575-GNVDE|56.95         |1889.5      |\n",
            "+----------+--------------+------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyT5QAhPzRQ0"
      },
      "source": [
        "from pyspark.sql import Row\r\n",
        "from pyspark.sql.types import StructField, StructType, StringType, LongType"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZkvWbw97S7Y",
        "outputId": "9b7cf6bd-9a2a-4591-a7a2-fee98bcfa080"
      },
      "source": [
        "#If you didn't set inderShema to True, here is what is happening to the type. There are all in string.\r\n",
        "df_string = spark.read.csv(\"/content/drive/MyDrive/data.csv\", header=True, inferSchema=  False)\r\n",
        "df_string.printSchema()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- customerID: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- SeniorCitizen: string (nullable = true)\n",
            " |-- Partner: string (nullable = true)\n",
            " |-- Dependents: string (nullable = true)\n",
            " |-- tenure: string (nullable = true)\n",
            " |-- PhoneService: string (nullable = true)\n",
            " |-- MultipleLines: string (nullable = true)\n",
            " |-- InternetService: string (nullable = true)\n",
            " |-- OnlineSecurity: string (nullable = true)\n",
            " |-- OnlineBackup: string (nullable = true)\n",
            " |-- DeviceProtection: string (nullable = true)\n",
            " |-- TechSupport: string (nullable = true)\n",
            " |-- StreamingTV: string (nullable = true)\n",
            " |-- StreamingMovies: string (nullable = true)\n",
            " |-- Contract: string (nullable = true)\n",
            " |-- PaperlessBilling: string (nullable = true)\n",
            " |-- PaymentMethod: string (nullable = true)\n",
            " |-- MonthlyCharges: string (nullable = true)\n",
            " |-- TotalCharges: string (nullable = true)\n",
            " |-- Churn: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xza9CkYjzj7e"
      },
      "source": [
        "schema = StructType(\r\n",
        "    [\r\n",
        "        StructField(name = \"customerID\", dataType= StringType(), nullable= True),\r\n",
        "        StructField(name = \"gender\", dataType= StringType(), nullable= True),\r\n",
        "        StructField(name = \"Partner\", dataType= StringType(), nullable= True),\r\n",
        "    ]\r\n",
        ")"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJOes0Chz1yu"
      },
      "source": [
        "rows = [\r\n",
        "        Row(\"7590-VHVEG\",\"Female\",\"Yes\"),\r\n",
        "        Row(\"5575-GNVDE\",\"Male\",\"No\"),\r\n",
        "        Row(\"3668-QPYBK\",\"Male\",\"No\"),\r\n",
        "]"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsbqqDd30kcW"
      },
      "source": [
        "parallelizeRows = spark.sparkContext.parallelize(rows)\r\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Int73XqK00r0"
      },
      "source": [
        "df = spark.createDataFrame(parallelizeRows,schema)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WA16SSU808UI",
        "outputId": "1fdf0c63-8c9a-4c35-d066-9a231aa30d3a"
      },
      "source": [
        "df.show()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+------+-------+\n",
            "|customerID|gender|Partner|\n",
            "+----------+------+-------+\n",
            "|7590-VHVEG|Female|    Yes|\n",
            "|5575-GNVDE|  Male|     No|\n",
            "|3668-QPYBK|  Male|     No|\n",
            "+----------+------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "5mgfvQEo74_Y",
        "outputId": "4c9e456c-a7c6-41ca-fd6a-41566a7cb1fb"
      },
      "source": [
        "#You can select and show the rows with select and the names of the features. Below, gender and churn are selected.\r\n",
        "df.select('gender','churn').show(5)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-3b916ef4df92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#You can select and show the rows with select and the names of the features. Below, gender and churn are selected.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gender'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'churn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.0.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1419\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m         \"\"\"\n\u001b[0;32m-> 1421\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1422\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`churn`' given input columns: [Partner, customerID, gender];;\n'Project [gender#381, 'churn]\n+- LogicalRDD [customerID#380, gender#381, Partner#382], false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWFEIEjf-H61"
      },
      "source": [
        "df = spark.read.csv(\"/content/drive/MyDrive/data.csv\", header =True)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-aXM9_x8BIU",
        "outputId": "e97ee57b-0998-4b9c-9ddf-040086a71079"
      },
      "source": [
        "#To get a summary statistics, of the data, you can use describe(). It will compute the :count, mean, standarddeviation, min, max\r\n",
        "df.describe().show()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----------+------+------------------+-------+----------+------------------+------------+-------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+--------------------+------------------+------------------+-----+\n",
            "|summary|customerID|gender|     SeniorCitizen|Partner|Dependents|            tenure|PhoneService|MultipleLines|InternetService|OnlineSecurity|OnlineBackup|DeviceProtection|TechSupport|StreamingTV|StreamingMovies|      Contract|PaperlessBilling|       PaymentMethod|    MonthlyCharges|      TotalCharges|Churn|\n",
            "+-------+----------+------+------------------+-------+----------+------------------+------------+-------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+--------------------+------------------+------------------+-----+\n",
            "|  count|      7043|  7043|              7043|   7043|      7043|              7043|        7043|         7043|           7043|          7043|        7043|            7043|       7043|       7043|           7043|          7043|            7043|                7043|              7043|              7043| 7043|\n",
            "|   mean|      null|  null|0.1621468124378816|   null|      null| 32.37114865824223|        null|         null|           null|          null|        null|            null|       null|       null|           null|          null|            null|                null| 64.76169246059922|2283.3004408418697| null|\n",
            "| stddev|      null|  null|0.3686116056100135|   null|      null|24.559481023094442|        null|         null|           null|          null|        null|            null|       null|       null|           null|          null|            null|                null|30.090047097678482| 2266.771361883145| null|\n",
            "|    min|0002-ORFBO|Female|                 0|     No|        No|                 0|          No|           No|            DSL|            No|          No|              No|         No|         No|             No|Month-to-month|              No|Bank transfer (au...|               100|                  |   No|\n",
            "|    max|9995-HOTOH|  Male|                 1|    Yes|       Yes|                 9|         Yes|          Yes|             No|           Yes|         Yes|             Yes|        Yes|        Yes|            Yes|      Two year|             Yes|        Mailed check|             99.95|             999.9|  Yes|\n",
            "+-------+----------+------+------------------+-------+----------+------------------+------------+-------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+--------------------+------------------+------------------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIYVXzMYsIQe"
      },
      "source": [
        "dataset = spark.read.csv(\"/content/drive/MyDrive/data.csv\",inferSchema=True, header =True)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccQW1Roy97p7",
        "outputId": "0877fbdf-9879-4ad2-bcb9-6c632f5c722d"
      },
      "source": [
        "#Distinct values for Categorical columns\r\n",
        "#The distinct() will come in handy when you want to determine the unique values in the categorical columns in the dataframe.\r\n",
        "\r\n",
        "df.select(\"PaymentMethod\").distinct().show()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|       PaymentMethod|\n",
            "+--------------------+\n",
            "|Credit card (auto...|\n",
            "|        Mailed check|\n",
            "|Bank transfer (au...|\n",
            "|    Electronic check|\n",
            "+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE7UbIriKBJk",
        "outputId": "a22a2d1c-1c2b-4891-c9a9-4f1eb587401c"
      },
      "source": [
        "from pyspark.sql import functions as F\r\n",
        "df.groupBy(\"tenure\").agg(F.sum(\"TotalCharges\")).show()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+------------------+\n",
            "|tenure| sum(TotalCharges)|\n",
            "+------+------------------+\n",
            "|     7| 54809.20000000001|\n",
            "|    51|231617.00000000006|\n",
            "|    15| 90158.60000000003|\n",
            "|    54|          270935.4|\n",
            "|    11| 64160.34999999998|\n",
            "|    69|463223.94999999995|\n",
            "|    29|126617.15000000002|\n",
            "|    42|180685.94999999998|\n",
            "|    64|         388536.85|\n",
            "|     3| 34938.00000000001|\n",
            "|    30|         151223.75|\n",
            "|    34|152946.19999999998|\n",
            "|    59|251270.10000000003|\n",
            "|     8| 56923.04999999999|\n",
            "|    22|122167.10000000003|\n",
            "|    28|107741.64999999998|\n",
            "|    16|           80233.0|\n",
            "|    52|         279499.75|\n",
            "|    35|         190918.95|\n",
            "|     0|              null|\n",
            "+------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkQsMkrhLpZX",
        "outputId": "c70b90a4-cfce-4073-9ad0-b2e6b237035f"
      },
      "source": [
        "df.count()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7043"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--5Cg7oOOG6Z",
        "outputId": "694c8813-7bb7-473f-87af-b61766166b0a"
      },
      "source": [
        "df.dtypes\r\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('customerID', 'string'),\n",
              " ('gender', 'string'),\n",
              " ('SeniorCitizen', 'string'),\n",
              " ('Partner', 'string'),\n",
              " ('Dependents', 'string'),\n",
              " ('tenure', 'string'),\n",
              " ('PhoneService', 'string'),\n",
              " ('MultipleLines', 'string'),\n",
              " ('InternetService', 'string'),\n",
              " ('OnlineSecurity', 'string'),\n",
              " ('OnlineBackup', 'string'),\n",
              " ('DeviceProtection', 'string'),\n",
              " ('TechSupport', 'string'),\n",
              " ('StreamingTV', 'string'),\n",
              " ('StreamingMovies', 'string'),\n",
              " ('Contract', 'string'),\n",
              " ('PaperlessBilling', 'string'),\n",
              " ('PaymentMethod', 'string'),\n",
              " ('MonthlyCharges', 'string'),\n",
              " ('TotalCharges', 'string'),\n",
              " ('Churn', 'string')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2MToGJ8K-eF",
        "outputId": "ea369815-2bdc-48fd-ad4d-0754ffe3ecb4"
      },
      "source": [
        "df.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns]).show()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+------+-------------+-------+----------+------+------------+-------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------+----------------+-------------+--------------+------------+-----+\n",
            "|customerID|gender|SeniorCitizen|Partner|Dependents|tenure|PhoneService|MultipleLines|InternetService|OnlineSecurity|OnlineBackup|DeviceProtection|TechSupport|StreamingTV|StreamingMovies|Contract|PaperlessBilling|PaymentMethod|MonthlyCharges|TotalCharges|Churn|\n",
            "+----------+------+-------------+-------+----------+------+------------+-------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------+----------------+-------------+--------------+------------+-----+\n",
            "|         0|     0|            0|      0|         0|     0|           0|            0|              0|             0|           0|               0|          0|          0|              0|       0|               0|            0|             0|           0|    0|\n",
            "+----------+------+-------------+-------+----------+------+------------+-------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------+----------------+-------------+--------------+------------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcm8vccCLIsA",
        "outputId": "68b2e945-5b33-4fb3-da3d-33f0fce0f688"
      },
      "source": [
        "from time import time\r\n",
        "t0 = time()\r\n",
        "all_raw_data = df.collect()\r\n",
        "tt = time() - t0\r\n",
        "print (\"Data collected in {} seconds\".format(round(tt,3)))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data collected in 0.55 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k36odz1RtYO4"
      },
      "source": [
        "df.write.csv(\"/content/drive/MyDrive/new_data1\")"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zmVWSmOEqaB"
      },
      "source": [
        "Map reduce example:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qMnKKNQ-mPZ",
        "outputId": "1e791ef0-9c88-451d-e85a-6536db04d56f"
      },
      "source": [
        "# For map reduce we need to create SparkContext so that we can read a textfile and perform mapping function on it. \r\n",
        "from pyspark import SparkContext\r\n",
        "sc = SparkContext.getOrCreate()\r\n",
        "text_file = sc.textFile(\"/content/drive/MyDrive/icp4.txt\")\r\n",
        "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\r\n",
        "             .map(lambda word: (word, 1)) \\\r\n",
        "             .reduceByKey(lambda a, b: a + b)\r\n",
        "for x in counts.collect():\r\n",
        "    print (x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('People', 1)\n",
            "('who', 1)\n",
            "('have', 1)\n",
            "('been', 1)\n",
            "('fully', 1)\n",
            "('vaccinated', 1)\n",
            "('against', 1)\n",
            "('coronavirus', 1)\n",
            "('--', 2)\n",
            "('right', 1)\n",
            "('now', 1)\n",
            "('that', 1)\n",
            "('means', 1)\n",
            "('with', 2)\n",
            "('two', 1)\n",
            "('doses', 1)\n",
            "('of', 1)\n",
            "('either', 1)\n",
            "('the', 4)\n",
            "('Pfizer/BioNTech', 1)\n",
            "('or', 1)\n",
            "('Moderna', 1)\n",
            "('vaccine', 1)\n",
            "('can', 1)\n",
            "('skip', 1)\n",
            "('quarantine', 1)\n",
            "('if', 1)\n",
            "('they', 2)\n",
            "('are', 1)\n",
            "('exposed', 1)\n",
            "('to', 2)\n",
            "('someone', 1)\n",
            "('infected', 1)\n",
            "('virus,', 1)\n",
            "('US', 1)\n",
            "('Centers', 1)\n",
            "('for', 2)\n",
            "('Disease', 1)\n",
            "('Control', 1)\n",
            "('and', 1)\n",
            "('Prevention', 1)\n",
            "('said', 1)\n",
            "('Wednesday.That', 1)\n",
            "(\"doesn't\", 1)\n",
            "('mean', 1)\n",
            "('should', 1)\n",
            "('stop', 1)\n",
            "('taking', 1)\n",
            "('precautions,', 1)\n",
            "('CDC', 1)\n",
            "('noted', 1)\n",
            "('in', 1)\n",
            "('updated', 1)\n",
            "('guidance.', 1)\n",
            "(\"It's\", 1)\n",
            "('just', 1)\n",
            "('not', 1)\n",
            "('necessary', 1)\n",
            "('them', 1)\n",
            "('quarantine.', 1)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}